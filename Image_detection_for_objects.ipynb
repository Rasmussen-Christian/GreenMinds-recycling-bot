{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Image_detection_for_objects.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RKoy7yD2FYEl","colab_type":"text"},"source":["#Imports"]},{"cell_type":"code","metadata":{"id":"YZWlzbb8E8IC","colab_type":"code","colab":{}},"source":["# imports related to PyTorch\n","import torch\n","from torch import nn, optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms, models\n","from collections import OrderedDict\n","\n","# python tools\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# other\n","import json\n","from PIL import Image\n","\n","# google collab specific\n","from google.colab import drive"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wU1ERTR9Fwtr","colab_type":"text"},"source":["#Loading of data\n","We are using `torchvision` to load the data ([documentation](http://pytorch.org/docs/0.3.0/torchvision/index.html)). The dataset is split into three parts, training, validation, and testing. For the training, there has been applyed transformations such as random scaling, cropping, and flipping. This will help the network generalize leading to better performance. You'll also need to make sure the input data is resized to 224x224 pixels as required by the pre-trained networks.\n","\n","The validation and testing sets are used to measure the model's performance on data it hasn't seen yet. For this we don't want any scaling or rotation transformations, but you'll need to resize then crop the images to the appropriate size.\n","\n","The pre-trained networks we'll use were trained on the ImageNet dataset where each color channel was normalized separately. For all three set we'll need to normalize the means and standard deviations of the images to what the network expects. For the means, it's [0.485, 0.456, 0.406] and for the standard deviations [0.229, 0.224, 0.225], calculated from the ImageNet images. These values will shift each color channel to be centered at 0 and range from -1 to 1."]},{"cell_type":"markdown","metadata":{"id":"sBKjNK6BF3ea","colab_type":"text"},"source":["#### Gets acces to the google drive folder\n","\\* only if running in google collab"]},{"cell_type":"code","metadata":{"id":"sETuvxkcF0MU","colab_type":"code","colab":{}},"source":["# gets acces to the google drive folder\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p2B3QYNNUkBL","colab_type":"text"},"source":["#### getting the data\n","we are using the data loader class which means that the images are arranged in this way:\n","\n","* class 'dog'\n","    * root/dog/xxx.png\n","    * root/dog/xxy.png\n","    * root/dog/xxz.png\n","* class 'cat'\n","    * root/cat/123.png\n","    * root/cat/nsdf3.png\n","    * root/cat/asd932_.png\n","\n"]},{"cell_type":"code","metadata":{"id":"QZ0RSSkEUlaS","colab_type":"code","colab":{}},"source":["# paths to data needs to get changed, for new data location \n","dir = '/content/gdrive/My Drive/Colab Notebooks/PyTorch-project/flowers'\n","\n","data_dir = {\n","    'train': '/train',\n","    'valid':'/valid',\n","    'test': '/test'   \n","}\n","\n","# Defines the transforms for the training, validation, and testing sets\n","data_transforms = {\n","    'train': transforms.Compose([\n","                transforms.RandomRotation(30),\n","                transforms.RandomResizedCrop(224),\n","                transforms.RandomHorizontalFlip(),\n","                transforms.ToTensor(),\n","                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","            ]),\n","    'valid': transforms.Compose([\n","                transforms.Resize(255),\n","                transforms.CenterCrop(224),\n","                transforms.ToTensor(),\n","                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","            ]),\n","    'test': transforms.Compose([\n","                transforms.Resize(255),\n","                transforms.CenterCrop(224),\n","                transforms.ToTensor(),\n","                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","            ])\n","}\n","\n","\n","# Loads the datasets with ImageFolder\n","data_datasets = {\n","    'train': datasets.ImageFolder(data_dir['train'], transform=data_transforms['train']),\n","    'valid': datasets.ImageFolder(data_dir['valid'], transform=data_transforms['valid']),\n","    'test': datasets.ImageFolder(data_dir['test'], transform=data_transforms['test'])\n","}\n","    \n","# Using the image datasets and the trainforms, defines the dataloaders\n","data_dataloaders = {\n","    'train': torch.utils.data.DataLoader(data_datasets['train'], batch_size=32, shuffle=True),\n","    'valid': torch.utils.data.DataLoader(data_datasets['valid'], batch_size=32),\n","    'test': torch.utils.data.DataLoader(data_datasets['test'], batch_size=32)\n","}"],"execution_count":0,"outputs":[]}]}
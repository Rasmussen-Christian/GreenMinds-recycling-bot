{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GreenMindsNN.ipynb","provenance":[],"collapsed_sections":["nZE_hWZZz__h","p1-NFtgsUyva","H0uGf7JyIkZ2"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"RKoy7yD2FYEl","colab_type":"text"},"source":["#Imports\n","Everyting with torch is for PyTorch which is our system for the neural network\n","\n","Numpy, pandas, and matlab is for structuring and visualisation of data\n","%matplotlib and %config is used to show graphs in line in the google collab view"]},{"cell_type":"code","metadata":{"id":"YZWlzbb8E8IC","colab_type":"code","colab":{}},"source":["# imports related to PyTorch\n","import torch\n","from torch import nn, optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms, models\n","from collections import OrderedDict\n","\n","# python tools\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","# other\n","import json\n","from PIL import Image\n","\n","# google collab specific\n","from google.colab import drive"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wU1ERTR9Fwtr","colab_type":"text"},"source":["#Loading of data\n","We are using `torchvision` to load the data ([documentation](http://pytorch.org/docs/0.3.0/torchvision/index.html)). The dataset is split into three parts, training, validation, and testing. For the training, there has been applyed transformations such as random scaling, cropping, and flipping. This will help the network generalize leading to better performance. You'll also need to make sure the input data is resized to 224x224 pixels as required by the pre-trained networks.\n","\n","The validation and testing sets are used to measure the model's performance on data it hasn't seen yet. For this we don't want any scaling or rotation transformations, but you'll need to resize then crop the images to the appropriate size.\n","\n","The pre-trained networks we'll use were trained on the ImageNet dataset where each color channel was normalized separately. For all three set we'll need to normalize the means and standard deviations of the images to what the network expects. For the means, it's [0.485, 0.456, 0.406] and for the standard deviations [0.229, 0.224, 0.225], calculated from the ImageNet images. These values will shift each color channel to be centered at 0 and range from -1 to 1."]},{"cell_type":"markdown","metadata":{"id":"sBKjNK6BF3ea","colab_type":"text"},"source":["#### Gets acces to the google drive folder\n","\\* only run if using google collab"]},{"cell_type":"code","metadata":{"id":"sETuvxkcF0MU","colab_type":"code","colab":{}},"source":["# gets acces to the google drive folder\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p2B3QYNNUkBL","colab_type":"text"},"source":["#### getting the data\n","we are using the data loader class from PyTorch which means that the images should be arranged following this structure:\n","\n","* class 'dog'\n","    * root/dog/xxx.png\n","    * root/dog/xxy.png\n","    * root/dog/xxz.png\n","* class 'cat'\n","    * root/cat/123.png\n","    * root/cat/nsdf3.png\n","    * root/cat/asd932_.png\n","\n"]},{"cell_type":"code","metadata":{"id":"QZ0RSSkEUlaS","colab_type":"code","colab":{}},"source":["# paths to data needs to get changed, for new data location \n","path = '/content/gdrive/My Drive/cs3_Rasmussen/Collab/_DATA/GreenMinds-recycling-data-2'\n","\n","data_dir = {\n","    'train': path + '/train',\n","    'valid': path+ '/valid',\n","    'test': path + '/test'   \n","}\n","\n","# Defines the transforms for the training, validation, and testing sets\n","data_transforms = {\n","    'train': transforms.Compose([\n","                transforms.RandomRotation(30),\n","                transforms.RandomResizedCrop(224),\n","                transforms.RandomHorizontalFlip(),\n","                transforms.ToTensor(),\n","                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","            ]),\n","    'valid': transforms.Compose([\n","                transforms.Resize(255),\n","                transforms.CenterCrop(224),\n","                transforms.ToTensor(),\n","                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","            ]),\n","    'test': transforms.Compose([\n","                transforms.Resize(255),\n","                transforms.CenterCrop(224),\n","                transforms.ToTensor(),\n","                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","            ])\n","}\n","\n","# Loads the datasets with ImageFolder\n","data_datasets = {\n","    'train': datasets.ImageFolder(data_dir['train'], transform=data_transforms['train']),\n","    'valid': datasets.ImageFolder(data_dir['valid'], transform=data_transforms['valid'])\n","    #'test': datasets.ImageFolder(data_dir['test'], transform=data_transforms['test'])\n","}\n","\n","# Using the image datasets and the trainforms, defines the dataloaders\n","# we are using batch_size of 32 to make sure not to run out of memory on google collab\n","data_dataloaders = {\n","    'train': torch.utils.data.DataLoader(data_datasets['train'], batch_size=32, shuffle=True),\n","    'valid': torch.utils.data.DataLoader(data_datasets['valid'], batch_size=32)\n","    #'test': torch.utils.data.DataLoader(data_datasets['test'], batch_size=32)\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FMYzAzS6W4Ah","colab_type":"text"},"source":["# Building the classifier"]},{"cell_type":"markdown","metadata":{"id":"gsbYMGm_W_Co","colab_type":"text"},"source":["Getting our pretrained model"]},{"cell_type":"code","metadata":{"id":"FM9_SYcBW514","colab_type":"code","colab":{}},"source":["model = models.densenet121(pretrained=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cVZulnh2QV6Q","colab_type":"text"},"source":["Saves our classs names to their model index for use later in inferince"]},{"cell_type":"code","metadata":{"id":"cqy7FZAVQWcg","colab_type":"code","colab":{}},"source":["model.class_to_idx = data_datasets['train'].class_to_idx"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J1v5kL6IXBOF","colab_type":"text"},"source":["Freezing the parameters so we wont backprop through them"]},{"cell_type":"code","metadata":{"id":"l3E2BUpZW7uq","colab_type":"code","colab":{}},"source":["for param in model.parameters():\n","    param.requires_gradu = False"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pUSni9cYXFRL","colab_type":"text"},"source":["Creating a new classifier for transfor learning"]},{"cell_type":"code","metadata":{"id":"I1Df1YD1W8AL","colab_type":"code","colab":{}},"source":["prediction_size = len(model.class_to_idx)\n","\n","classifier = nn.Sequential(OrderedDict([\n","    ('fc_1', nn.Linear(1024, 512)),\n","    ('relu_1', nn.ReLU()),\n","    ('dropout_1', nn.Dropout(.2)),\n","    ('fc_2', nn.Linear(512, 256)),\n","    ('relu_2', nn.ReLU()),\n","    ('dropout_2', nn.Dropout(.2)),\n","    ('fc_3', nn.Linear(256, prediction_size)),\n","    ('output', nn.LogSoftmax(dim=1))\n","]))\n","\n","model.classifier = classifier"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ieJGINOwXJ17","colab_type":"text"},"source":["Checks if cuda is enabled, and sends our model to the gpu if avaliable"]},{"cell_type":"code","metadata":{"id":"b6-wWRRWW8MH","colab_type":"code","colab":{}},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZCSwlVKjXMpY","colab_type":"text"},"source":["Defining hyper parameters"]},{"cell_type":"code","metadata":{"id":"etCKx2ZOW8X1","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Hyper parameters \n","criterion = nn.NLLLoss()\n","optimizer = optim.Adam(model.classifier.parameters(), lr=0.001, weight_decay=0.1)\n","\n","epochs =  2#@param {type:\"integer\"}\n","steps = 0\n","print_every_step =  10#@param {type:\"integer\"}\n","\n","auto_save = True #@param {type:\"boolean\"}\n","save_path = '/content/gdrive/My Drive/cs3_Rasmussen/Collab/_DATA/GreenMinds-recycling-data-2/checkpoint_03_22.pth' #@param {type:\"string\"}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bvyZ2rX0XQCu","colab_type":"text"},"source":["Creating our training loop"]},{"cell_type":"code","metadata":{"id":"ftmXa4SSXOSc","colab_type":"code","colab":{}},"source":["# sets our model to train mode\n","model.train()\n","# sends our model to the device (cpu/gpu)\n","model = model.to(device)\n","\n","# records the best accuracy to auto save best model\n","best_accuracy = 0\n","\n","# for graphing\n","train_losses, test_losses = [], []\n","\n","# runs through set amount of epochs\n","for epoch in range(epochs):\n","    # holds our runnning loss\n","    running_loss = 0\n","\n","    # loops over our train data\n","    for inputs, labels in data_dataloaders['train']:\n","        # add 1 to the number of steps done\n","        steps += 1\n","        \n","        # moves input- and labels- tensors to the device (cpu/gpu)\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        \n","        # resets our optimizer\n","        optimizer.zero_grad()\n","        # gets our log probabilities from our model\n","        logps = model.forward(inputs)\n","        # calculates the loss using our criterion\n","        loss = criterion(logps, labels)\n","        # backprobagates to update our weights\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # adds our loss to the total loss, between validation\n","        running_loss += loss.item()\n","        \n","        # calculates our models current accuracy each x steps\n","        if steps % print_every_step == 0:\n","            # holds our test loss\n","            test_loss = 0\n","            # holds our total accuracy\n","            accuracy = 0\n","            \n","            # sets our model to eval, to stop dropout\n","            model.eval()\n","            \n","            # stops our optimizer for better performance\n","            with torch.no_grad():\n","                # loops through our validation data\n","                for inputs, labels in data_dataloaders['valid']:\n","                    # moves input- and labels- tensors to the device (cpu/gpu)\n","                    inputs, labels = inputs.to(device), labels.to(device)\n","                    \n","                    # gets our log probabilities from our model\n","                    logps = model.forward(inputs)\n","                    # calculates the loss using our criterion\n","                    batch_loss = criterion(logps, labels)\n","                    # adds our loss to the total loss validation loss\n","                    test_loss += batch_loss.item()\n","                    \n","                    # ---Calcualtes the accuracy\n","                    # converts to probabilities\n","                    ps = torch.exp(logps)\n","                    # gets the top prediction\n","                    top_p, top_class = ps.topk(1, dim=1)\n","                    # Check if it is equal to the label\n","                    equals = top_class == labels.view(*top_class.shape)\n","                    # adds our accuracy this step to the total validation accuracy\n","                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n","            \n","            train_losses.append(running_loss/print_every_step)\n","            test_losses.append(test_loss/len(data_dataloaders['valid']))\n","            \n","            # Prints out data about our current model accuracy and losses\n","            print(f\"Epoch {epoch+1}/{epochs}.. \"\n","                  f\"Train loss: {running_loss/print_every_step:.3f}.. \"\n","                  f\"Test loss: {test_loss/len(data_dataloaders['valid']):.3f}.. \"\n","                  f\"Test accuracy: {accuracy/len(data_dataloaders['valid']):.10f}\")\n","            \n","            # saves the model if it has improved and auto save is on\n","            if ((accuracy/len(data_dataloaders['valid'])) > best_accuracy) and auto_save:\n","                best_accuracy = accuracy/len(data_dataloaders['valid'])\n","                save_model_full(model, model.class_to_idx, save_path)\n","                print(f\"Model with accuracy of {accuracy/len(data_dataloaders['valid']):.3f} saved\")\n","\n","\n","            # resets our running loss\n","            running_loss = 0\n","            # resets our model to train mode\n","            model.train()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cqii0h_nc6Up","colab_type":"text"},"source":["#### Plots training vs validation loss\n","used to check for over / under fitting"]},{"cell_type":"code","metadata":{"id":"Dg6ZoFqmo31O","colab_type":"code","cellView":"both","colab":{}},"source":["plt.plot(train_losses, label='Training loss')\n","plt.plot(test_losses, label='Validation loss')\n","plt.ylim(bottom=0)\n","plt.legend(frameon=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nZE_hWZZz__h","colab_type":"text"},"source":["# Validation on the test set"]},{"cell_type":"code","metadata":{"id":"OITJ0AD9z__i","colab_type":"code","colab":{}},"source":["# holds our total test loss\n","test_loss = 0\n","# holds our total accuracy\n","accuracy = 0\n","# sends our model to the device (cpu/gpu)\n","model = model.to(device)\n","# sets our model to eval mode\n","model.eval()\n","\n","# stops our optimizer for better performance\n","with torch.no_grad():\n","    # loops over our test data\n","    for inputs, labels in data_dataloaders['test']:\n","        # moves input- and labels- tensors to the device (cpu/gpu)\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        \n","        # gets our log probabilities from our model\n","        logps = model.forward(inputs)\n","        \n","        # --- Calcualtes the accuracy\n","        # converts to probabilities\n","        ps = torch.exp(logps)\n","        # gets the top prediction\n","        top_p, top_class = ps.topk(1, dim = 1)\n","        # Check if it is equal to the label\n","        equals = top_class == labels.view(*top_class.shape)\n","        # adds our accuracy this step to the total test accuracy\n","        accuracy += torch.mean(equals.type(torch.FloatTensor))\n","\n","# prints out the final result\n","print('Test accuracy: {}'.format(accuracy / len(data_dataloaders['test'])))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p1-NFtgsUyva","colab_type":"text"},"source":["# Saving / loading the model"]},{"cell_type":"code","metadata":{"id":"0f9NlJYxU04n","colab_type":"code","colab":{}},"source":["def save_model(model, epoch, class_to_idx, path):\n","    '''\n","        saves model state dict for loading later, \n","        to load this saved model is an internet connecting recuired to load a densenet model\n","    '''\n","\n","    torch.save({\n","        'model_state_dict' : model.state_dict(),\n","        'epochs' : epoch,\n","        'class_to_idx' : class_to_idx\n","    }, path)\n","\n","def save_model_full(model, class_to_idx, path):\n","    '''\n","        Saves a full model so it can be loaded without an internet connection\n","    '''\n","    torch.save({\n","            'model': model,\n","            'class_to_idx': class_to_idx\n","            }, path)\n","\n","\n","#save_model(model, 2, model.class_to_idx, '/content/gdrive/My Drive/cs3_Rasmussen/Collab/_DATA/GreenMinds-recycling-data/densenet_checkpoint.pth')\n","#save_model_full(model, model.class_to_idx, '/content/gdrive/My Drive/cs3_Rasmussen/Collab/_DATA/GreenMinds-recycling-data/densenet_checkpoint_full.pth')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yxMNV6tSsTln","colab_type":"code","colab":{}},"source":["def load_model(path):\n","    '''\n","        loads a preetrained model of type densenet121\n","        an internet connection is required to load the densenet model\n","    '''\n","    # loads the data\n","    checkpoint = torch.load(path)\n","\n","    # downloads the pre trained model\n","    model = models.densenet121(pretrained=True)\n","    for param in model.parameters():\n","        param.requires_gradu = False\n","\n","    # re-creates the classifier\n","    prediction_size = len(checkpoint['class_to_idx'])\n","    classifier = nn.Sequential(OrderedDict([\n","        ('fc_1', nn.Linear(1024, 512)),\n","        ('relu_1', nn.ReLU()),\n","        ('dropout_1', nn.Dropout(.2)),\n","        ('fc_2', nn.Linear(512, 256)),\n","        ('relu_2', nn.ReLU()),\n","        ('dropout_2', nn.Dropout(.2)),\n","        ('fc_3', nn.Linear(256, prediction_size)),\n","        ('output', nn.LogSoftmax(dim=1))\n","    ]))\n","    model.classifier = classifier\n","\n","    # loads the old weights\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    # stores the class_to_idx\n","    model.class_to_idx = checkpoint['class_to_idx']\n","    \n","    return model\n","\n","def load_model_full(path):\n","    '''\n","        loads a model and its custom class_to_idx var, without internet requirement\n","    '''\n","    # loads the data\n","    checkpoint = torch.load(path)\n","\n","    model = checkpoint['model']\n","    model.class_to_idx = checkpoint['class_to_idx']\n","\n","    return model\n","\n","\n","#model = load_model('/content/gdrive/My Drive/cs3_Rasmussen/Collab/_DATA/GreenMinds-recycling-data/densenet_checkpoint.pth')\n","#model = load_model_full('/content/gdrive/My Drive/cs3_Rasmussen/Collab/_DATA/GreenMinds-recycling-data/densenet_checkpoint_full.pth')\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H0uGf7JyIkZ2","colab_type":"text"},"source":["# Sanity check"]},{"cell_type":"code","metadata":{"id":"QOWDXtjIz___","colab_type":"code","cellView":"form","colab":{}},"source":["def process_image(image):\n","    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n","        returns an Numpy array\n","    '''\n","    # creates the pre proccessing transforms\n","    resize = transforms.Compose([\n","        transforms.Resize(255),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n","    ])\n","    # applying transforms and returns the image\n","    return resize(image)\n","\n","def imshow(image, ax=None, title=None):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","    if ax is None:\n","        fig, ax = plt.subplots()\n","    \n","    # PyTorch tensors assume the color channel is the first dimension\n","    # but matplotlib assumes is the third dimension\n","    image = image.numpy().transpose((1, 2, 0))\n","    \n","    # Undo preprocessing\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    image = std * image + mean\n","    \n","    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n","    image = np.clip(image, 0, 1)\n","    \n","    ax.axis('off')\n","    ax.set_title(title)\n","    ax.imshow(image)\n","    \n","    return ax\n","\n","def predict(image_path, model, device, topk=5):\n","    ''' Predict the class (or classes) of an image using a trained deep learning model.\n","    '''\n","    # opens the image and applies preproccessing\n","    image = process_image(Image.open(image_path))\n","    # unsqueezes it\n","    image = image.unsqueeze_(0)\n","    # sends it to our device (cpu/gpu)\n","    image = image.to(device)\n","    model = model.to(device)\n","    \n","    # sets our model to eval mode, to disable dropout\n","    model.eval()\n","    # stops our optimizer for better performance\n","    with torch.no_grad():\n","        # gets our log probabilities from our model\n","        logps = model.forward(image)\n","\n","    # converts to probabilities\n","    ps = torch.exp(logps)\n","    \n","    # Get the top 5 probabilities and classes\n","    prop, classes = ps.topk(topk, dim=1)\n","\n","    # Get the first items in the tensor list which contains the probs and classes\n","    top_p = prop.tolist()[0]\n","    top_classes = classes.tolist()[0]\n","\n","    # sets up a lost to hold our labels\n","    labels = []\n","\n","    # reverses our class_to_idx which our model holds\n","    idx_to_class = {v : k for k, v in model.class_to_idx.items()}\n","\n","    # loops through each prediction to find the labels instead of numbers\n","    for c in top_classes:\n","        # adds the name which our model did predict\n","        labels.append(idx_to_class[c])\n","\n","    # returns our top k probabilities and labels as lists.\n","    return top_p, labels\n","\n","def sanity_check(test_image_path, test_image_title, model, device):\n","    '''\n","        Prints image to test, and resoults from model\n","    '''\n","    # opens the image and applies preproccessing\n","    proccessed_image = process_image(Image.open(test_image_path))\n","    # displays the image with the correct label\n","    ax = imshow(proccessed_image, title=test_image_title)\n","\n","    # Gets our top  image prediction from our model\n","    probs, classes = predict(test_image_path, model, device, 4)\n","\n","    # creates a new fig\n","    fig = plt.figure(figsize=(4, 7))\n","    # adds a subplot for our graph\n","    ax2 = fig.add_subplot(2, 1 , 2)\n","    # creates the graph\n","    ax2.barh(classes, probs)\n","\n","#@title Sanity check image { display-mode: \"form\" }\n","g_class = 'WaterBottle' #@param [\"Calculator\", \"ExpoMarkerBlue\", \"PureLeaf\", \"WaterBottle\"] {allow-input: true}\n","\n","sanity_check('/content/gdrive/My Drive/cs3_Rasmussen/Collab/_DATA/GreenMinds-recycling-data/valid/' + g_class + '/image_25.jpg', g_class, model, device)"],"execution_count":0,"outputs":[]}]}